\chapter{Large-Scale Linear Optimization}
\lecture{16}{1 Nov. 08:00}{Large-Scale Linear Optimization}

Let's first look at an example.
\begin{eg}
	Consider a constraint matrix
	\[
		\left(\begin{array}{cccccc}
				[\quad\quad\quad\quad                            &                                              &                                                        &  &        & \quad\ \quad]                                     \\
				\left[\begin{array}{cc} & \\ &\end{array}\right] &                                              &                                                        &  &        &                                                   \\
				                                                 & \left[\begin{array}{cc} & \end{array}\right] &                                                        &  & 0      &                                                   \\
				                                                 &                                              & \left[\begin{array}{cc} & \\ & \\ & \end{array}\right] &  &        &                                                   \\
				                                                 & 0                                            &                                                        &  & \ddots                                                     \\
				                                                 &                                              &                                                        &  &        & \left[\begin{array}{cc} & \\ & \end{array}\right]
			\end{array}\right)
	\]

	We see that if the first constraint(the first row) doesn't exist, then the problem decomposes to
	those small block matrix corresponds to some smaller, easier linear optimization problems, and we can solve it very quickly.

	\begin{note}
		We called the above matrix as \textbf{Nearly Separates}.
	\end{note}
\end{eg}

There is something we need in order to solve the above problem.

\section{Decomposition Algorithm}
In this section we describe what is usually known as \textbf{Dantzig-Wolfe Decomposition}. We need
\begin{enumerate}
	\item \hyperref[algo:simplex-algorithm]{Simplex Algorithm}.
	\item Geometry of \hyperref[def:basic-solution]{basic} \hyperref[def:feasible-solution]{feasible solutions} and \hyperref[def:basic-direction]{directions}.
	\item Duality.
\end{enumerate}

We first see a useful theorem.
\subsection{Representation Theorem}
Let \((P)\) be
\begin{align*}
	\min~    & c^{\top}x \\
	         & Ax = b    \\
	(P)\quad & x\geq 0.
\end{align*}

\begin{theorem}[Representation theorem]\label{thm:representation-theorem}
	Suppose that \((P)\) is feasible. Then let \(\mathcal{X}\) be
	\[
		\mathcal{X} \coloneqq \{\hat{x}^j \colon j\in \mathcal{J}\}
	\]
	be the set of \hyperref[def:basic-solution]{basic} \hyperref[def:feasible-solution]{feasible solutionss} of \((P)\). Also, let \(\mathcal{Z}\) be
	\[
		\mathcal{Z}\coloneqq \{\hat{z}^k\colon k\in \mathcal{K}\}
	\]
	be the set of basic feasible \hyperref[def:ray]{rays} of \((P)\).

	Then the \hyperref[def:feasible-region]{feasible region} of \((P)\) is equal to
	\[
		S^\prime\coloneqq \left\{\sum\limits_{j\in\mathcal{J}} \lambda_{j} \hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k\colon \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1;\ \lambda_{j}\geq 0,\ j\in \mathcal{J};\ \mu^k\geq 0,\ k\in \mathcal{K}\right\}.
	\]
\end{theorem}
\begin{proof}
	Let \(S\) be the \hyperref[def:feasible-region]{feasible region} of \((P)\). We show that \(S = S^\prime\) by showing \(S^\prime\subseteq S\) and \(S^\prime\supseteq S\).
	\begin{enumerate}
		\item \(S^\prime\subseteq S\). Since
		      \[
			      A\left(\sum\limits_{j}\lambda_{j}\hat{x}^j + \sum\limits_{K}\mu_K \hat{z}^K \right)= \sum\limits_j \lambda_{j}\underbrace{\left(A \hat{x}^j\right)}_{=b}+\sum\limits_{K} \mu^K \underbrace{\left(A \hat{z}^K\right)}_{=0} = b.
		      \]
		      Moreover, since everything in the sum is non-negative, we see that \(S^\prime\subseteq S\).
		\item \(S\subseteq S^\prime\). Assume \(\hat{x}\in S\). Then consider the following system
		      \[
			      \begin{split}
				      \substack{n+1\\ \text{ equations}}&\begin{dcases}
					      \sum\limits_{j\in\mathcal{J}} \lambda_{j}\hat{x}^j + \sum\limits_{k\in\mathcal{K} } \mu_k \hat{z}^k & = \hat{x} \\
					      \sum\limits_{j}\lambda_{j}                                                                          & = 1       \\
				      \end{dcases}\\
				      (I)\quad&\lambda_{j}\geq 0 \text{ for }j\in\mathcal{J};\ \mu^k\geq 0 \text{ for }k\in\mathcal{K}.
			      \end{split}
		      \]
		      \begin{note}
			      Keep in mind that in the above system, \(\hat{x}\) and \(\hat{z}\) are fixed, the variables are the \(\lambda_{j}\) and \(\mu_k\).
		      \end{note}

		      Now, instead of directly constructing a solution, we use \hyperref[lma:Farkas-lemma]{Farkas' Lemma}. Namely, we write down a system such that
		      if this system is infeasible, by \hyperref[lma:Farkas-lemma]{Farkas' Lemma}, our original system is feasible. Firstly, in
		      \hyperref[lma:Farkas-lemma]{Farkas' Lemma}, we have
		      \[
			      A = \begin{pmatrix}
				      \hat{x}^1 & \hat{x}^2 & \ldots & \hat{z}^1 & \hat{z}^2 & \hat{z}^3 & \ldots \\
				      1         & 1         & \ldots & 0         & 0         & \ldots    & 0      \\
			      \end{pmatrix},\qquad b = \begin{pmatrix}
				      \hat{x} \\
				      1       \\
			      \end{pmatrix}
		      \]
		      in \((I)\). Now, denote the \hyperref[def:dual]{dual} variables with \(w,\ t\), then we have
		      \[
			      \begin{split}
				      \begin{pmatrix}
					      w^{\top} & t \\
				      \end{pmatrix}\begin{pmatrix}
					      \hat{x} \\
					      1       \\
				      \end{pmatrix}&>0\\
				      \begin{pmatrix}
					      w^{\top} & t \\
				      \end{pmatrix}\begin{pmatrix}
					      \hat{x}^j \\
					      1         \\
				      \end{pmatrix}&\leq 0 \text{ for }j\in\mathcal{J}\\
				      \begin{pmatrix}
					      w^{\top} & t \\
				      \end{pmatrix}\begin{pmatrix}
					      \hat{z}^k \\
					      0         \\
				      \end{pmatrix}&\leq 0 \text{ for }k\in\mathcal{K}\\
			      \end{split}
		      \]
		      for \((II)\). We only need to show that \((II)\) cannot have a solution. This is easy to show. Firstly,
		      we see that the above inequalities are equivalent to
		      \[
			      \begin{alignedat}{3}
				      &w^{\top}\hat{x}+t&&>0\\
				      &w^{\top}\hat{x}^j + t&&\leq 0\\
				      &w^{\top}\hat{z}^k &&\leq \vec{0}
			      \end{alignedat}\iff\begin{alignedat}{3}
				      &-w^{\top}\hat{x}&&<t\\
				      &-w^{\top}\hat{x}^j &&\geq \hat{t} &&\text{ for }j\in\mathcal{J}\\
				      &-w^{\top}\hat{z}^K &&\geq 0 &&\text{ for }k\in\mathcal{K}.
			      \end{alignedat}
		      \]
		      Now, suppose this does have a solution \(\hat{w}, \hat{t}\). Then, consider
		      \begin{align*}
			      \min~ & -\hat{w}^{\top}x(<\hat{t}) \\
			            & Ax = b                     \\
			            & x\geq 0.
		      \end{align*}
		      Notice that the objective value of \(\hat{x}\) here is less than \(\hat{t}\) by \((II)\). Since we know that
		      \(Ax = b\), hence this linear programming is feasible. Moreover, from \(-\hat{w}^{\top}\hat{x}\leq \hat{t}\) and
		      \(-\hat{w}^{\top}\hat{x}^j\geq \hat{t}\), we see that we have a better solution with respect to the \hyperref[def:objective-function]{objective function}
		      among the linear combination of \emph{extreme points} \(\hat{x}^j\). But this is only possible for unbounded linear
		      programming problem, which needs the positive dot product between rays and the objective vector. But from
		      \(-\hat{w}\hat{z}^k\geq 0\), we see that this will never happen, hence the theorem is proved.
		      \begin{figure}[H]
			      \centering
			      \incfig{representation-theorem}
			      \caption{Bounded and unbounded case in \hyperref[algo:simplex-algorithm]{Simplex Algorithm}}
			      \label{fig:representation-theorem}
		      \end{figure}
	\end{enumerate}
\end{proof}

With this \hyperref[thm:representation-theorem]{representation theorem}, consider
\begin{align*}
	\min~         & c^{\top}x     \\
	              & Ex\geq h      \\
	\text{"easy"} & \begin{dcases}
		                Ax = b \\
		                x\geq 0.
	                \end{dcases}
\end{align*}

Then by
\[
	\left\{\underbrace{\sum\limits_{j\in\mathcal{J}} \lambda_{j} \hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k}_{=\left\{x\in\mathbb{\MakeUppercase{R}}^n\colon Ax = b,\ x\geq \vec{0}\right\}}
	\colon \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1, \lambda_{j}\geq 0 \text{ for }j\in \mathcal{J}, \mu^k\geq 0\text{ for } k\in \mathcal{K}\right\},
\]
we turn the linear problem into
\begin{align*}
	\min~ & c^{\top}\left(\sum\limits_{j\in\mathcal{J}}\lambda_{j}\hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k  \right) \\
	      & E\left(\sum\limits_{j\in\mathcal{J}}\lambda_{j}\hat{x}^j + \sum\limits_{k\in\mathcal{K}}\mu_k \hat{z}^k  \right) \geq h \\
	      & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                            \\
	      & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_K\geq 0 \text{ for }k\in\mathcal{K}.
\end{align*}

Furthermore, this is equivalent to
\begin{align*}
	\min~    & \sum\limits_{j\in\mathcal{J}}\left(c^{\top}\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(c^{\top} \hat{z}^k  \right)\mu_k \\
	         & \sum\limits_{j\in\mathcal{J}}\left(E\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(E \hat{z}^k \right)\mu_k \geq h         \\
	         & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                                                 \\
	(M)\quad & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_k\geq 0 \text{ for }k\in\mathcal{K}.
\end{align*}
The system is now extremely reduced, but the cost is that we now have huge amount of variables. We call this as the \hyperref[def:master-problem]{master problem}.

\begin{definition}[Master problem]\label{def:master-problem}
	Given a linear programming problem
	\begin{align*}
		\min~         & c^{\top}x     \\
		              & Ex\geq h      \\
		\text{"easy"} & \begin{dcases}
			                Ax = b \\
			                x\geq 0,
		                \end{dcases}
	\end{align*}
	the so-called \emph{master problem} is defined as
	\begin{align*}
		\min~    & \sum\limits_{j\in\mathcal{J}}\left(c^{\top}\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(c^{\top} \hat{z}^k  \right)\mu_k \\
		         & \sum\limits_{j\in\mathcal{J}}\left(E\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(E \hat{z}^k \right)\mu_k \geq h         \\
		         & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                                                 \\
		(M)\quad & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_k\geq 0 \text{ for }k\in\mathcal{K}.
	\end{align*}
\end{definition}
We formalize the above result as so-called \hyperref[thm:decomposition-theorem]{Decomposition Theorem}.
\begin{theorem}[Decomposition theorem]\label{thm:decomposition-theorem}
	Let
	\begin{align*}
		\min~    & c^{\top}x \\
		         & Ex \geq h \\
		         & Ax = b    \\
		(Q)\quad & x\geq 0
	\end{align*}
	Let \(S\coloneqq \left\{ x\in\mathbb{\MakeUppercase{R}}^n\colon Ax = b, x\geq 0 \right\}\), \(\mathcal{X} \coloneqq \left\{\hat{x}^j\colon j\in\mathcal{J} \right\}\) be
	the set of \hyperref[def:basic-solution]{basic} \hyperref[def:feasible-solution]{feasible solutions} \(S\) and
	\(\mathcal{Z} \coloneqq \left\{\hat{z}^k\colon k\in\mathcal{K} \right\}\) be the set of basic feasible \hyperref[def:ray]{rays} of \(S\). Then \(Q\) is equivalent to
	the \hyperref[def:master-problem]{master problem} \((M)\)
	\begin{align*}
		\min~    & \sum\limits_{j\in\mathcal{J}}\left(c^{\top}\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(c^{\top} \hat{z}^k  \right)\mu_k \\
		         & \sum\limits_{j\in\mathcal{J}}\left(E\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(E \hat{z}^k \right)\mu_k \geq h         \\
		         & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                                                 \\
		(M)\quad & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_k\geq 0 \text{ for }k\in\mathcal{K}.
	\end{align*}
\end{theorem}

\begin{remark}
	We think of \(E\) being a \emph{complicated} constraint matrix, while \(A\) is much easier. Further, the reason why we choose \(\leq \) for \(E\) and \(=\) for \(A\) is not because
	this makes them complicated or easy, but only for our convenience. In deed, we will soon see that we can turn \(M\) into a \hyperref[def:standard-form]{standard form} problem
	without increasing complexity.
\end{remark}

\section{Solution of the Master Problem via the Simplex Algorithm}
We now want to solve \(M\). And since we can't write out \(M\) explicitly since there are too many variables. But instead, we can reasonably
\emph{maintain} a \hyperref[def:basic-solution]{basic solution} of \(\overline{M}\), the \hyperref[def:standard-form]{standard form} of \(M\).
Furthermore, the only part of the \hyperref[algo:simplex-algorithm]{simplex algorithm} that is sensitive to the total number of variables is
when we check for variables with negative \hyperref[def:reduced-cost]{reduced cost}. So we now try to find an indirect way to check this rather than find it one by one.

Denotes the \hyperref[def:dual]{dual} variable of \(M\) as \(y\) and \(\sigma\) with \(y\geq \vec{0}\) and \(\sigma\) unrestricted. We further turn \(M\) into
the \hyperref[def:standard-form]{standard form} problem, which is just
\begin{align*}
	\min~               & \sum\limits_{j\in\mathcal{J}}\left(c^{\top}\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(c^{\top} \hat{z}^k  \right)\mu_k \\
	                    & \sum\limits_{j\in\mathcal{J}}\left(E\hat{x}^j\right)\lambda_{j} + \sum\limits_{k\in\mathcal{K}}\left(E \hat{z}^k \right)\mu_k - Is = h       \\
	                    & \sum\limits_{j\in\mathcal{J}}\lambda_{j} = 1                                                                                                 \\
	(\overline{M})\quad & \lambda_{j}\geq 0 \text{ for }j\in\mathcal{J},\ \mu_K\geq 0 \text{ for }k\in\mathcal{K}, s\geq 0.
\end{align*}

Suppose that \(\overline{y}\), \(\overline{\sigma}\) forms a \hyperref[def:dual-basic-solution]{basic dual solution}. The \hyperref[def:reduced-cost]{reduced cost}
of \(\lambda_{j}\) associated with \(\hat{x}^j\) is
\[
	(c^{\top}\hat{x}^j) - \begin{pmatrix}
		\overline{y}^{\top} & \overline{\sigma} \\
	\end{pmatrix}\begin{pmatrix}
		E\hat{x}^j \\
		1          \\
	\end{pmatrix} = c^{\top}\hat{x}^j - \hat{y}^{\top}E\hat{x}^j - \overline{\sigma} = (c^{\top} - \overline{y}^{\top}E)\hat{x}^j - \overline{\sigma}
\]
since \(\overline{c}_{\eta_{j}} = c_{\eta_{j}} - \overline{y}^{\top}A_{\eta_{j}}\).

\begin{problem}
Is there a \(\lambda_{j}\) with this \hyperref[def:reduced-cost]{reduced cost} negative?
\end{problem}
\begin{answer}
	Consider
	\begin{align*}
		-\sigma + \min~ & (c^{\top} - \overline{y}^{\top}E)x \\
		                & Ax = b                             \\
		                & x\geq 0.
	\end{align*}
\end{answer}

