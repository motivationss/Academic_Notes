\lecture{10}{4 Oct. 08:00}{Complementary}
%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
\section{Complementary}
\begin{definition}[Complementary]\label{def:complementary}
	Solutions \(\hat{x}\) to \((P)\) and \(\hat{y}\) to \((D)\) are \emph{complementary} if
	\[
		m+n\text{ equations}\begin{cases}
			(\underbrace{c_{j} - \hat{y}^{\top} A_{\cdot j}}_{ = 0\text{ for }j\in \beta}) \underbrace{\hat{x}_j}_{ \substack{= 0 \\\text{ for }\\j\in \eta}} = 0, & j = 1\cdots n; \\\\
			\hat{y}_i(\underbrace{A_{i\cdot} \hat{x} - b_{i}}_{ = 0 \text{ for }\overline{x}}) = 0, & i = 1\cdots m.
		\end{cases}
	\]
\end{definition}

Now, suppose we have a \hyperref[def:basic-partition]{basic partition} \(\beta, \eta\) such that
\[
	\begin{split}
		\overline{x}&\colon \overline{x}_{\beta} = A^{-1}_{\beta}b,\ \overline{x}_{\eta} = \vec{0}\\
		\overline{y}&\colon \overline{y}^{\top} = c^{\top}_{\beta}A^{-1}_{\beta}.
	\end{split}
\]

\begin{note}
	Specifically, we see that \(c_{j} - \hat{y}^{\top}A_{\cdot j} = 0\) for \(j\in\beta\) is because \(\overline{y}^{\top} = c^{\top}_{\beta}A^{-1}_{\beta}\), and then
	\[
		c_{j} - \hat{y}^{\top}A_{\cdot j} = c_{j} - c^{\top}_{\beta}\underbrace{A^{-1}_{\beta}A_{\cdot j}}_{e_{j}} = c_{j} - c_{j} = 0.
	\]
\end{note}

Then just from above, we see that the following theorems hold.
\begin{theorem}\label{thm:lec10-1}
	If \(\overline{x}\) and \(\overline{y}\) are \hyperref[def:basic-solution]{basic solutions} for \(\beta, \eta\), then \(\overline{x}\) and \(\overline{y}\) are
	\hyperref[def:complementary]{complementary}.
\end{theorem}

\begin{theorem}[Complementary with equal objective value]\label{thm:complementary-with-equal-objective-value}
	If \(\hat{x}\) and \(\hat{y}\) are \hyperref[def:complementary]{complementary}, then
	\[
		c^{\top}\hat{x} = \hat{y}^{\top} b.
	\]
\end{theorem}
\begin{note}
	\[
		c^{\top}_{\beta} A^{-1}_{\beta}b = \overline{y}^{\top} b,\qquad c^{\top}(A^{-1}_{\beta} b) = c^{\top}_{\beta}\overline{x}_{\beta} = c^{\top}\overline{x}.
	\]
\end{note}
\begin{explanation}
	We show that
	\[
		c^{\top} \hat{x} - \hat{y}^{\top} b = 0.
	\]
	We have
	\[
		\begin{split}
			c^{\top} \hat{x} - \hat{y}^{\top} b &= (c^{\top} - \underbrace{\hat{y}^{\top}A)\hat{x} + \hat{y}^{\top}(A \hat{x}}_{\text{added terms}} - b)\\
			&=\sum\limits_{j=1}^{n} \underbrace{(c_{j}-\hat{y}^{\top}A_{\cdot j})x_{j}}_{ = 0\text{ for }i = 1\ldots n}
			+ \sum\limits_{i=1}^{m} \underbrace{\hat{y}_i(A_{i\cdot}\hat{x} - b_{i})}_{ = 0\text{ for }i = 1\ldots m}\\
			&= 0.
		\end{split}
	\]
\end{explanation}

\begin{theorem}[Weak complementary slackness theorem]\label{thm:weak-complementary-slackness-theorem}
	If \(\hat{x}\) and \(\hat{y}\) are \hyperref[def:feasible-solution]{feasible} and \hyperref[def:complementary]{complementary}, then they are \hyperref[def:optimal-solution]{optimal}.
\end{theorem}
\begin{proof}
	Follows from \autoref{thm:weak-duality-theorem} and \hyperref[def:complementary]{complementary solutions} having equal objective value from \autoref{thm:complementary-with-equal-objective-value}.
\end{proof}

\begin{theorem}[Strong complementary slackness theorem]\label{thm:strong-complementary-slackness-theorem}
	If \(\hat{x}\) and \(\hat{y}\) are \hyperref[def:optimal-solution]{optimal}, then \(\hat{x}\) and \(\hat{y}\) are \hyperref[def:complementary]{complementary}.
\end{theorem}
\begin{proof}
	Recall that
	\[
		\overbrace{\sum\limits_{j=1}^{n} \underbrace{(c_{j}-\hat{y}^{\top}A_{\cdot j})}_{ \geq 0\text{ for each }j}\underbrace{\hat{x}_{j}}_{\geq 0 \text{ for each }j}}^{\cancel{\geq 0}\implies = 0\text{ for each }j}
		+ \overbrace{\sum\limits_{i=1}^{m} \underbrace{\hat{y}_i(A_{i\cdot}\hat{x} - b_{i})}_{ = 0\text{ for each }i}}^{=0}
		\underbrace{= 0 = c^{\top}\hat{x} - \hat{y}^{\top}b}_{\substack{\text{if }\hat{x}\text{ and }\hat{y}\text{ are \hyperref[def:optimal-solution]{optimal}:}\\ \text{same object value}}}
	\]
	Hence, the equality can only hold if
	\[
		(c_{j} - \hat{y}^{\top}A_{\cdot j})\hat{x}_j = 0, \text{ for }j = 1, 2, \ldots , n;
	\]
	with the obvious fact that
	\[
		\hat{y}_i(A_{i\cdot}\hat{x} - b_{i}) = 0, \text{ for }i = 1, 2, \ldots , m,
	\]
	so they are \hyperref[def:complementary]{complementary}.
\end{proof}

%────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
\section{Duality for General Linear Optimization Problems}
So far, we only discuss the \hyperref[def:dual]{dual} of the \hyperref[def:standard-form]{standard form} problem. But we will see that \emph{every}
linear optimization problem has a natural \hyperref[def:dual]{dual}.

Now consider a \hyperref[def:general-linear-programming-problem]{general linear programming problem}
\begin{align*}
	\min~              & c^{\top}_P x_P + c^{\top}_N x_N + c^{\top}_U x_U \\
	                   & A_{GP}x_P +A_{GN}x_N + A_{GU}x_U\geq b_G         \\
	                   & A_{LP}x_P +A_{LN}x_N + A_{LU}x_U \leq b_L        \\
	                   & A_{EP}x_P +A_{EN}x_N + A_{EU}x_U = b_E           \\
	(\mathcal{G})\quad & x_P\geq 0, x_N \leq 0, x_U \text{ unrestricted}.
\end{align*}

We first turn this into a \hyperref[def:standard-form]{standard form} problem:
\begin{enumerate}
	\item \(\widetilde{x}_N \coloneqq -x_N\):
	      \begin{align*}
		      \min~ & c^{\top}_P x_P + c^{\top}_N x_N + c^{\top}_u x_U \\
		            & A_{GP}x_P -A_{GN}x_N + A_{GU}x_U\geq b_G         \\
		            & A_{LP}x_P -A_{LN}x_N + A_{LU}x_U \leq b_L        \\
		            & A_{EP}x_P -A_{EN}x_N + A_{EU}x_U = b_E           \\
		            & x_P\geq 0, x_N \leq 0, x_U \text{ unrestricted}
	      \end{align*}
	\item \(x_U = \widetilde{x}_U - \widetilde{\widetilde{x}}_U\), where \(\widetilde{x}_U, \widetilde{\widetilde{x}}_U \geq 0\):
	      \begin{align*}
		      \min~ & c^{\top}_P x_P + c^{\top}_N x_N + c^{\top}_U \widetilde{x}_U - c_U \widetilde{\widetilde{x}}_U \\
		            & A_{GP}x_P -A_{GN}x_N + A_{GU}\widetilde{x}_U -A_{GU}\widetilde{\widetilde{x}}_U \geq b_G       \\
		            & A_{LP}x_P -A_{LN}x_N + A_{LU}\widetilde{x}_U -A_{LU}\widetilde{\widetilde{x}}_U\leq b_L        \\
		            & A_{EP}x_P -A_{EN}x_N + A_{EU}\widetilde{x}_U -A_{EU}\widetilde{\widetilde{x}}_U= b_E           \\
		            & x_P\geq 0, x_N \leq 0, \widetilde{x}_U \geq 0, \widetilde{\widetilde{x}}_U\geq 0
	      \end{align*}
	\item Adding \hyperref[def:slack-variable]{slack variables}:
	      \[
		      \begin{alignedat}{3}
			      \min~ & c^{\top}_P x_P + c^{\top}_N x_N + c^{\top}_U \widetilde{x}_U - c_U \widetilde{\widetilde{x}}_U    \\
			      & A_{GP}x_P -A_{GN}x_N + A_{Gu}\widetilde{x}_U -A_{GU}\widetilde{\widetilde{x}}_U - s_G &       && = b_G \\
			      & A_{LP}x_P -A_{LN}x_N + A_{Lu}\widetilde{x}_U -A_{LU}\widetilde{\widetilde{x}}_U       & + t_L && = b_L \\
			      & A_{EP}x_P -A_{EN}x_N + A_{Eu}\widetilde{x}_U -A_{EU}\widetilde{\widetilde{x}}_U       &       && = b_E \\
			      & x_P\geq 0, x_N \leq 0, \widetilde{x}_U \geq 0, \widetilde{\widetilde{x}}_U\geq 0, s_G\geq 0, t_L \geq 0
		      \end{alignedat}
	      \]
\end{enumerate}

With \emph{\hyperref[def:dual]{dual} variables} \(y_G, y_L, y_E\), we have
\[
	\begin{alignedat}{4}
		\max~ & y^{\top}_G b_G &&+ y^{\top}_L b_L &&+ y^{\top}_E b_E                     \\
		& y^{\top}_G A_{GP} &&+ y^{\top}_L A_{LP} &&+ y^{\top}_E A_{EP} &&\leq c^{\top}_P   \\
		-& y^{\top}_G A_{GN} &&- y^{\top}_L A_{LN} &&- y^{\top}_E A_{EN} &&\leq -c^{\top}_N \\
		& y^{\top}_G A_{GU} &&+ y^{\top}_L A_{LU} &&+ y^{\top}_E A_{EU} &&\leq c^{\top}_U   \\
		-& y^{\top}_G A_{GU} &&- y^{\top}_L A_{LU} &&- y^{\top}_E A_{EU} &&\leq -c^{\top}_U \\
		&y^{\top}_G \geq 0&&, y^{\top}_L\leq 0.
	\end{alignedat}.
\]
We time \(-1\) to the both side of the second constraint, and we see that last two \hyperref[def:structured-constraints]{structure constraints} can be reduced to a
single equality, results in
\[
	\begin{alignedat}{4}
		\max~ & y^{\top}_G b_G &&+ y^{\top}_L b_L &&+ y^{\top}_E b_E                     \\
		& y^{\top}_G A_{GP} &&+ y^{\top}_L A_{LP} &&+ y^{\top}_E A_{EP} &&\leq c^{\top}_P   \\
		& y^{\top}_G A_{GN} &&+ y^{\top}_L A_{LN} &&+ y^{\top}_E A_{EN} &&\geq c^{\top}_N \\
		& y^{\top}_G A_{GU} &&+ y^{\top}_L A_{LU} &&+ y^{\top}_E A_{EU} && = c^{\top}_U     \\
		(\mathcal{H})\quad &y^{\top}_G \geq 0&&, y^{\top}_L\leq 0.
	\end{alignedat}
\]

Finally, we remark that this gives us a simple result as we have already seen before.
\begin{theorem*}[Duality for general LP]
	We rephrase the weak and strong duality theorem in a more general term.
	\begin{theorem}[Weak duality theorem]\label{thm:weak-duality-theorem-general-LP}
		If \((\hat{x}_P, \hat{x}_N, \hat{x}_U)\) is \hyperref[def:feasible-solution]{feasible} in \(\mathcal{G}\) and the \hyperref[def:dual]{dual} variables
		\((\hat{y}_G, \hat{y}_L, \hat{y}_E)\) is \hyperref[def:feasible-solution]{feasible} in \(\mathcal{H}\), then
		\[
			c^{\top}_P \hat{x}_P + c^{\top}_N \hat{x}_N + c^{\top}_U \hat{x}_U \geq \hat{y}^{\top}_G b_G + \hat{y}^{\top}_L b_L + \hat{y}^{\top}_E b_E.
		\]
	\end{theorem}
	\begin{theorem}[Strong duality theorem]\label{thm:strong-duality-theorem-general-LP}
		If \(\mathcal{G}\) has a \hyperref[def:feasible-solution]{feasible solution}, and \(\mathcal{G}\) is not unbounded, then there exist \hyperref[def:feasible-solution]{feasible solutions}
		\((\hat{x}_P, \hat{x}_N, \hat{x}_U)\) for \(\mathcal{G}\) and \((\hat{y}_G, \hat{y}_L, \hat{y}_E)\) for \(\mathcal{H}\) that are \hyperref[def:optimal-solution]{optimal}.
		Moreover,
		\[
			c^{\top}_P \hat{x}_P + c^{\top}_N \hat{x}_N + c^{\top}_U \hat{x}_U = \hat{y}^{\top}_G b_G + \hat{y}^{\top}_L b_L + \hat{y}^{\top}_E b_E.
		\]
	\end{theorem}
\end{theorem*}

\begin{remark}
	We can also rephrase the \autoref{thm:weak-complementary-slackness-theorem} and \autoref{thm:strong-complementary-slackness-theorem}
	in this setup. The proof follows the same idea, but with some more works.
\end{remark}