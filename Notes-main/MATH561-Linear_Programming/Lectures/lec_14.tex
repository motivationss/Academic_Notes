\chapter{Sensitivity Analysis}
\lecture{14}{25 Oct. 08:00}{Sensitivity Analysis}
As usual, we start with the \hyperref[def:primal]{primal} and the \hyperref[def:dual]{dual}
\[
	\begin{alignedat}{5}
		\min~&c^{\top}x\qquad\qquad&&\max ~&&y^{\top}b\\
		&Ax = b 				&&		&&y^{\top}A\leq c^{\top}\\
		(P)\quad	&x\geq  0 	&&(D)\quad&&
	\end{alignedat}
\]
with an \hyperref[def:optimal-solution]{optimal} \hyperref[def:basic-partition]{basic partition} \(\beta, \eta\) such that
\[
	\overline{x} \coloneqq \begin{dcases}
		\overline{x}_{\beta} \coloneqq A_\beta^{-1} b \geq  \vec{0} \\
		\overline{x}_{\eta} \coloneqq \vec{0}
	\end{dcases}, \qquad \overline{y}^{\top} \coloneqq c_{\beta}^{\top}A^{-1}_{\beta}.
\]

\begin{prev}
	The \hyperref[def:dual]{dual} feasibility is
	\[
		\overline{c}_{\eta} \coloneqq c_{\eta} - c_{\beta}^{\top}A_{\beta}^{-1}A_{\eta} = c_{\eta} - \overline{y}^{\top}A_{\eta}\geq \vec{0}
	\]
	from \autoref{lma:lec7-2}.
\end{prev}

\section{Local Analysis}
\subsection{Change \(b\) on the right-hand side}
We let
\[
	b\to b+\Delta_i e_i = \begin{pmatrix}
		b_1            \\
		b_2            \\
		\vdots         \\
		b_{i}+\Delta_i \\
		\vdots         \\
		b_m            \\
	\end{pmatrix},
\]
then
\[
	A_{\beta}^{-1}(b+\Delta_{i}e_{i}) = A_{\beta}^{-1}b +\Delta_{i}\underbrace{A_{\beta}^{-1}e_{i}}_{h^{i}},
\]
where \(h_{i}\) is the \(i^{th}\) column of \(A^{-1}_{\beta}\). So now we have
\[
	\overline{x}_{\beta} + \Delta_{i}h^i \geq \vec{0},
\]
where we need \(\beta, \eta\) to still be an \hyperref[def:optimal-solution]{optimal} \hyperref[def:basic-partition]{partition}.

\subsection{Objective Value}
Now, the objective value is
\[
	c_{\beta}^{\top}(\overline{x}_{\beta}+\Delta_{i}A_{\beta}^{-1}e_{i})+ c_{\eta}^{\top}\vec{0} = \underbrace{c_{\beta}^{\top}\overline{x}_{\beta}}_{\substack{\text{old obj.}\\\text{value}}}+\Delta_{i}\underbrace{c_{\beta}^{\top}A_{\beta}^{-1}}_{\overline{y}^{\top}}e_{i} = c_{\beta}^{\top}\overline{x}_{\beta}+\Delta_{i}\overline{y}^{\top}_i.
\]

\subsection{Analysis}
Let \(f\) be
\begin{align*}
	f(b)\coloneqq \min~ & c^{\top}x \\
	                    & Ax = b    \\
	(P_b)\quad          & x\geq 0
\end{align*}
where
\[
	f: \mathbb{\MakeUppercase{R}}^{m}\to \mathbb{\MakeUppercase{R}}.
\]

We see that since the \hyperref[def:optimal-solution]{optimal} objective value is equal for the \hyperref[def:dual]{dual} of \(P_b\), then \(f(b) = y^{\top}b\). Then
\[
	\frac{\partial f}{\partial b_{i}} = \overline{y}_i
\]
if \(\overline{x}_{\beta}>\vec{0}\).

\begin{problem}
For what values of \(\Delta_{i}\) is
\[
	\overline{x}_{\beta} + \Delta_{i}h^i \geq \vec{0}?
\]
\end{problem}
\begin{answer}
	Firstly, we see that we need
	\[
		\overline{x}_{\beta_K}+\Delta_{i}h^i_K \geq 0 \text{ for }K = 1, \ldots , m.
	\]

	Equivalently,
	\[
		\Delta_{i}h^i_K \geq -\overline{x}_{\beta_K},
	\]
	hence
	\[
		\begin{dcases}
			\Delta_{i}\geq \frac{-\overline{x}_{\beta_K}}{h^i_K}, & \text{ if }h^i_K>0, \\
			\Delta_{i}\leq \frac{-\overline{x}_{\beta_K}}{h^i_K}, & \text{ if }h^i_K<0.
		\end{dcases}
	\]

	We define \(L_{i}, U_{i}\) such that
	\[
		L_{i}\leq \Delta_{i}\leq U_{i}
	\]
	where
	\[
		L_{i} \coloneqq \max_{K\colon h^i_K > 0}\{-\overline{x}_{\beta_K}/h^i_K\},\qquad U_{i} \coloneqq \min_{K\colon h^i_K < 0}\{-\overline{x}_{\beta_K}/h^i_K\}.
	\]

	\textbf{Reality Check}. We see that
	\[
		L_{i}\leq 0\leq U_{i}.
	\]

	\begin{remark}
		Noting that if \(h^i_K\leq 0\) for all \(K\), then we define \(L_{i} \coloneqq -\infty \). Similarly, if \(h^i_K\geq 0\) for all \(K\), we define \(U_i \coloneqq \infty \).
	\end{remark}
\end{answer}

\section{Global Analysis}
We start with a theorem.
\begin{theorem}\label{thm:lec14-1}
	The domain of \(f\) is a \hyperref[def:convex-set]{convex set}.
\end{theorem}
\begin{proof}
	Assume that the \hyperref[def:dual]{dual} of \(P_b\) is \hyperref[def:feasible-solution]{feasible}, where we denote the \hyperref[def:dual]{dual} as \(D_b\):
	\begin{align*}
		\max~      & y^{\top}b               \\
		(D_b)\quad & y^{\top}A\leq c^{\top}.
	\end{align*}

	Now, the domain is the set of \(b\) such that \(P_b\) is \hyperref[def:feasible-solution]{feasible}. Mathematically,
	\[
		S\coloneqq \left\{ b\colon Ax = b, x\geq 0 \text{ are feasible.}\right\}\subseteq \mathbb{\MakeUppercase{R}}^m.
	\]
	Suppose \(b^1, b^2\in S\). We want to check
	\[
		\lambda b^{1}+(1-\lambda) b^2 \in S\text{ for }0<\lambda<1.
	\]
	Notice that there is an \(x^1\) such that
	\[
		Ax^1 = b^1, x^1\geq \vec{0}
	\]
	and there is an \(x^2\) such that
	\[
		Ax^2 = b^2, x^2\geq \vec{0}.
	\]

	Firstly, we check that \(\lambda x^{1}+(1 - \lambda)x^2\) is non-negative. This is clear since all components are non-negative. Then we check
	\[
		A(\lambda x^{1}+(1 - \lambda)x^2) = \lambda b^1 + (1 - \lambda)b^2.
	\]
	This is clear since
	\[
		A(\lambda x^{1}+(1 - \lambda)x^2) = \lambda Ax^1 + (1 - \lambda)Ax^2 = \lambda b^1 + (1-\lambda)b^2.
	\]
\end{proof}

We now introduce the convexity of a function.
\begin{definition}[Convex function]\label{def:convex-function}
	We say that \(f\) is a \emph{convex function} on a \hyperref[def:convex-set]{convex domain} \(S\) if
	\[
		x^1, x^2\in S\text{ and }0<\lambda<1,
	\]
	then
	\[
		f(\lambda x^1+(1 - \lambda)x^2)\leq \lambda f(x^1)+(1-\lambda)f(x^2).
	\]
	\begin{figure}[H]
		\centering
		\incfig{convex-function}
		\label{fig:convex-function}
	\end{figure}
\end{definition}

\subsection{Affine Function}
Before we go further, we need to have several definitions.
\begin{definition}[Affine function]\label{def:affine-function}
	A function \(f\colon \mathbb{\MakeUppercase{R}}^{m}\to \mathbb{\MakeUppercase{R}}\) is \emph{affine} if
	\[
		f(u_1, u_2, \ldots , u_m) = a_0 + \sum\limits_{i=1}^{m} a_{i}u_{i}
	\]
	for \(a_i \in\mathbb{\MakeUppercase{R}},\ i = 0, \ldots , m\).
\end{definition}

\begin{remark}
	If \(a_0 = 0\), then \(f\) is a linear function.
\end{remark}

\begin{definition}[Convex piece-wise linear function]\label{def:convex-piece-wise-linear-function}
	A function \(f\colon \mathbb{\MakeUppercase{R}}^{m}\to \mathbb{\MakeUppercase{R}}\) is a \emph{convex piece-wise linear function} if
	\(f\) is the point-wise maximum of \hyperref[def:affine-function]{affine functions}.
	\begin{figure}[H]
		\centering
		\incfig{convex-piecewise-linear-function}
		\label{fig:convex-piecewise-linear-function}
	\end{figure}
\end{definition}

Now, suppose \(f_{i}\colon \mathbb{\MakeUppercase{R}}^m \to \mathbb{\MakeUppercase{R}}\) for \(i = 1, \ldots , K\) and assume that each is \hyperref[def:affine-function]{affine}.
Then define
\[
	f(x)\coloneqq \max_{1\leq i\leq K} \left\{ f_i(x) \right\}.
\]

\begin{theorem}
	The point-wise maximum of \hyperref[def:affine-function]{affine function} is a \hyperref[def:convex-function]{convex function}.
\end{theorem}
\begin{proof}
	We see that
	\[
		\begin{split}
			f(\lambda x^1 + (1 - \lambda)x^2) &=\max_{1\leq i\leq K}\left\{ f_i(\lambda x^1 + (1 - \lambda)x^2) \right\}\\
			&=\max_{1\leq i\leq K}\left\{ \lambda f_{i}(x^1) + (1 - \lambda)f_{i}(x^2) \right\}\\
			&\geq \max_{1\leq i\leq K}\left\{ \lambda f_{i}(x^1)\right\} + \max_{1\leq i\leq K}\left\{(1 - \lambda)f_{i}(x^2) \right\}\\
			&=\lambda\max_{1\leq i\leq K}\left\{f_{i}(x^1)\right\} + (1 - \lambda)\max_{1\leq i\leq K}\left\{f_{i}(x^2) \right\}
			= \lambda f(x^1)+(1 - \lambda)f(x^2).
		\end{split}
	\]
	\begin{remark}
		The second equality holds since
		\[
			\begin{split}
				&\max_{1\leq i\leq K}\left\{ a_{i0}+\sum\limits_{l=1}^{m} a_{il}(\lambda u^1_l + (1 - \lambda)u^2_l) \right\}\\
				= &\max_{1\leq i\leq K}\left\{ \lambda a_{i0}+(1 - \lambda)a_{i0} + \sum\limits_{l=1}^{m} a_{il}(\lambda u^1_l + (1 - \lambda)u^2_l) \right\}
			\end{split}
		\]
	\end{remark}
\end{proof}