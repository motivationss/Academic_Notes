\lecture{19}{10 Nov. 12:30}{Nash Equilibrium}
Besides the \hyperref[rationality]{rationality} assumptions, we also have the following.
\begin{note}[Bounded rationality]
	The \emph{bounded \hyperref[rationality]{rationality}} assumption says that \hyperref[def:player]{players} can only have
	some finite levels of \hyperref[rationality]{rationality}, i.e., not fully \hyperref[rationality]{rational}.
\end{note}

\begin{prev}
	Let's review notions we introduced before.
	\begin{itemize}
		\item \(\mathcal{I} \): the set of \hyperref[def:player]{players}.
		\item \(I\): \(\left\vert \mathcal{I}  \right\vert \), the number of \hyperref[def:player]{players}.
		\item \(\mathcal{S}_{i}\): \hyperref[def:player]{player} \(i\) choose \hyperref[def:strategy]{strategy} \(s_i\) from \(\mathcal{S}_{i}\) such that
		      \(s_{i}\in \mathcal{S}_i\) with \(S_{i} = \left\vert \mathcal{S}_i \right\vert\).
		\item \(s\): \((s_1, s_2, \ldots , s_I)\), \hyperref[def:strategy]{strategy} chosen by all \hyperref[def:player]{players}.
		\item \hyperref[def:player]{Player} \(i\in \mathcal{I}\), then \(-i\coloneqq \mathcal{I}\setminus\{i\}\): Everyone else but \(i\).
		      \begin{eg}
			      \[
				      \begin{split}
					      i = 1\colon\quad& -1 = \{2, 3, \ldots , I\},\\
					      i = 2\colon\quad& -2 = \{1, 3, \ldots , I\},\\
					      &\vdots
				      \end{split}
			      \]
			      since
			      \[
				      s_{-i} = (s_1, \ldots , s_{i - 1}, s_{i + 1}, \ldots , s_{I}).
			      \]
		      \end{eg}
		\item \hyperref[def:best-response]{Best-response correspondence} (set): For \hyperref[def:player]{player} \(i\), give the \hyperref[def:strategy]{strategies}
		      of \(-i\) (\(s_{-i} \) is given), find the subset of \hyperref[def:strategy]{strategies} in \(\mathcal{S}_i \) that give \hyperref[def:player]{player} \(i\)
		      the highest \hyperref[def:reward]{payoff}.
		\item \hyperref[def:reward]{Utilities}: \(\forall i\in \mathcal{I} \),
		      \[
			      u_{i}\colon \prod\limits_{j = 1}^{I} \mathcal{S}_{j}\to \mathbb{\MakeUppercase{R}}.
		      \]
		      For every vector \(s = (s_1, s_2, \ldots , s_I)\in \prod\limits_{j\in \mathcal{I}} \mathcal{S}_j\), \(u_{i}(s)\)
		      is a real number.
	\end{itemize}
	\begin{remark}
		We see that \(s_{i}, s_{-i}\) form the entire vector of \hyperref[def:strategy]{strategies} every \hyperref[def:player]{player} plays, so
		\(u_{i}(s_{i}, s_{-i})\) is \textbf{well-defined}. Moreover,
		\[
			\max_{s_{i}\in\mathcal{S}_i}u_{i}(s_{i}, s_{-i})
		\]
		finds all \hyperref[def:strategy]{strategies} that attain the maximum \hyperref[def:reward]{rewards}.
	\end{remark}
\end{prev}

\begin{prev}
	We now view the \hyperref[eg:golden-ball]{Golden Balls game} with what we just defined. The \hyperref[def:payoff-matrix]{payoff matrix} is
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|}
			                                               & \multicolumn{1}{c}{} & \multicolumn{2}{c}{\hyperref[def:player]{Player} 2}                                      \\
			                                               & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\text{split}$}                  & \multicolumn{1}{c}{$\text{steal}$} \\\cline{3-4}
			\multirow{2}*{\hyperref[def:player]{Player} 1} & $\text{split}$       & $(x/2, x/2)$                                        & $(0, x)$                           \\\cline{3-4}
			                                               & $\text{steal}$       & $(x, 0)$                                            & $(0, 0)$                           \\\cline{3-4}
		\end{tabular}
	\end{table}

	For \hyperref[def:player]{player} 1, given
	\begin{enumerate}
		\item \(s_{-1} = \text{steal}\):
		      \begin{itemize}
			      \item \(u_1(\text{split} , \underbrace{\text{steal}}_{s_{-1}} ) = 0\)
			      \item \(u_1(\text{steal} , \underbrace{\text{steal}}_{s_{-1}} ) = 0\)
		      \end{itemize}
		      So
		      \[
			      \arg\max_{s_1\in \mathcal{S}_1} u_1(s_1, \text{steal} ) = \{\text{split}, \text{steal}  \},
		      \]
		      which is our \hyperref[def:best-response]{best response correspondence}.
		\item \(s_{-1} = \mathrm{split}\):
		      \begin{itemize}
			      \item \(u_1(\text{split} , \underbrace{\text{split}}_{s_{-1}} ) = \frac{x}{2}\)
			      \item \(u_1(\text{steal} , \underbrace{\text{split}}_{s_{-1}} ) = x\)
		      \end{itemize}
		      So
		      \[
			      \arg\max_{s_1\in \mathcal{S}_1} u_1(s_1, \text{split} ) = \{\text{steal}\},
		      \]
		      which is our \hyperref[def:best-response]{best response correspondence}, and moreover, since it's unique and strictly greater
		      than any other choices, so we call it the \hyperref[def:strict-best-response]{strict best response}.
	\end{enumerate}
\end{prev}

\begin{definition}[Strict best response]\label{def:strict-best-response}
	\(\overline{s}_{i}\) is called a \emph{strict best response} to \(s_{-i}\) if
	\[
		u_{i}(\overline{s}_i, s_{-i})>u_{i}(s_{i}, s_{-i})
	\]
	for every \(s_{i}\in \mathcal{S}_{i}\setminus\{\overline{s}_i\} \).
\end{definition}

\begin{note}
	Obviously, a \hyperref[def:strict-best-response]{strict best response} will be unique. Also, note that \autoref{def:strict-best-response}
	is just \autoref{def:dominant-strategy}.
\end{note}

\begin{remark}
	With the introduced notations, we can reformulate \autoref{def:dominant-strategy} and \autoref{def:weakly-dominant-strategy} as follows.
	\(s_{i}\) is \emph{strictly dominant} if it is the \hyperref[def:strict-best-response]{strict best response} for all \(s_{-i}\), while
	\(s_{i}\) is \emph{weakly dominant} if it is one of the \hyperref[def:best-response]{best response} for all \(s_{-i}\).
\end{remark}

\begin{remark}
	We see that
	\begin{itemize}
		\item In the \hyperref[eg:golden-ball]{Golden balls} example, where \(\text{steal}\) is a \hyperref[def:weakly-dominant-strategy]{weakly dominant strategy}.
		\item \hyperref[rationality]{Rationality} plus \hyperref[common-knowledge]{common knowledge} implies that if a \hyperref[def:player]{player} with
		      \hyperref[def:dominant-strategy]{dominant strategies}, then the \hyperref[def:player]{player} will play them.
	\end{itemize}
\end{remark}

We can also reformulate \hyperref[def:Nash-equilibrium]{Nash equilibrium} with the introduced notions.

\begin{prev}[Nash equilibrium]
	Let \(s^{\ast} = (s_1^{\ast}, s_2^{\ast}, \ldots , s_I^{\ast})\) be the \textbf{strategy profile}, which is a \hyperref[def:pure-strategy]{pure strategy}
	\hyperref[def:Nash-equilibrium]{Nash equilibrium} if
	\[
		s_{i}^{\ast}\in\mathrm{BR}(s_{-i}^{\ast})
	\]
	for all \(i\in \mathcal{I} \).
\end{prev}

\begin{note}
	We see that
	\begin{itemize}
		\item Every \hyperref[def:player]{agent} is playing a \(\mathrm{BR}\) to the others.
		\item \hyperref[def:reward]{Utility} is the incentive for \hyperref[def:player]{agents}. Setting everyone else at \(s^{\ast}_{-i}\), there is no
		      incentive for \hyperref[def:player]{player} \(i\) to deviate (not necessary to change \hyperref[def:strategy]{action}). This implies there are no
		      \emph{unilateral dominants} are possible.
		      \[
			      \underset{i\in \mathcal{I}}{\forall}\ \underset{s_{i}\in \mathcal{S}_i}{\forall}\ u_{i}(s^{\ast}_i, s^{\ast}_{-i}) \geq u_{i}(s_{i}, s^{\ast}_{-i}).
		      \]
	\end{itemize}
\end{note}

\begin{prev}
	There are two different types of \hyperref[def:strategy]{strategies}:
	\begin{itemize}
		\item \hyperref[def:pure-strategy]{Pure strategy}: Fixing an \hyperref[def:strategy]{action} for every \hyperref[def:player]{player}.
		\item \hyperref[def:mixed-strategy]{Mixed strategy}: A randomized \hyperref[def:strategy]{action} is played by every \hyperref[def:player]{player}.
	\end{itemize}
\end{prev}

\begin{remark}
	We see that
	\begin{enumerate}
		\item \hyperref[def:pure-strategy]{Pure strategy} \hyperref[def:Nash-equilibrium]{Nash equilibrium} needs not exist for every \hyperref[def:mathematical-game]{game}.
		\item Even if they exist, they need not be unique.
		\item If there are multiple equilibrium, then which one gets played is a tough question and usually involves external quantities (outside information).
	\end{enumerate}
\end{remark}

\subsection{Pure Strategy Nash Equilibrium}
We first introduce the concept of \hyperref[def:coordination-game]{coordination games}.

\begin{definition}[Coordination game]\label{def:coordination-game}
	\emph{Coordination games} are \hyperref[def:game]{games} such that \hyperref[def:player]{players} get a higher \hyperref[def:reward]{payoff} by working
	together (taking the same \hyperref[def:strategy]{action}).
\end{definition}

\begin{eg}[Shaking hands]\label{eg:shaking-hands}
	There are two men want to shake their hands. They can either position their hand up (\(U\)) or down (\(D\)). The \hyperref[def:payoff-matrix]{payoff matrix}
	is
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|}
			                        & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Player 2}                           \\
			                        & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$U$}      & \multicolumn{1}{c}{$D$} \\\cline{3-4}
			\multirow{2}*{Player 1} & $U$                  & $(1, 1)$                     & $(0, 0)$                \\\cline{3-4}
			                        & $D$                  & $(0, 0)$                     & $(1, 1)$                \\\cline{3-4}
		\end{tabular}
	\end{table}
	We see that \((U, U)\) and \((D, D)\) are \hyperref[def:pure-strategy]{pure strategy} \hyperref[def:Nash-equilibrium]{Nash equilibrium}.
\end{eg}

\begin{eg}[Battle of the sexes]\label{eg:battle-of-the-sexes}
	A man and a woman are going to a date. The man prefer to see football (\(\mathrm{F}\)) while the woman prefer to go to theater (\(\mathrm{T}\)). If
	their opinion are not equal, then they can't go to anywhere, so their fulfillment (\hyperref[def:reward]{payoff}) will both be \(0\).
	The \hyperref[def:payoff-matrix]{payoff matrix}
	is
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|}
			                   & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Woman}                                           \\
			                   & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\mathrm{T}$} & \multicolumn{1}{c}{$\mathrm{F}$} \\\cline{3-4}
			\multirow{2}*{Man} & $\mathrm{T}$         & $(1, 2)$                         & $(0, 0)$                         \\\cline{3-4}
			                   & $\mathrm{F}$         & $(0, 0)$                         & $(2, 1)$                         \\\cline{3-4}
		\end{tabular}
	\end{table}
	We see that \((\mathrm{T}, \mathrm{T})\) and \((\mathrm{F}, \mathrm{F})\) are \hyperref[def:pure-strategy]{pure strategy} \hyperref[def:Nash-equilibrium]{Nash equilibrium}.
\end{eg}

\begin{eg}[Stay hunt]\label{eg:stay-hunt}
	The \hyperref[def:payoff-matrix]{payoff matrix} is
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|}
			                        & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Player 1}                                        \\
			                        & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\mathrm{S}$} & \multicolumn{1}{c}{$\mathrm{H}$} \\\cline{3-4}
			\multirow{2}*{Player 2} & $\mathrm{S}$         & $(4, 4)$                         & $(0, 3)$                         \\\cline{3-4}
			                        & $\mathrm{H}$         & $(3, 0)$                         & $(3, 3)$                         \\\cline{3-4}
		\end{tabular}
	\end{table}
	We see that \((\mathrm{S}, \mathrm{S})\) and \((\mathrm{H}, \mathrm{H})\) are \hyperref[def:pure-strategy]{pure strategy} \hyperref[def:Nash-equilibrium]{Nash equilibrium}.
\end{eg}

We also have so-called \hyperref[def:anti-coordination-game]{Anti-Coordination Games}.
\begin{definition}[Anti-coordination game]\label{def:anti-coordination-game}
	A  \hyperref[def:game]{game} is an \emph{anti-coordination game} if one \hyperref[def:player]{player} prefers to coordinate, and the other does not.
\end{definition}

And before we see examples about \hyperref[def:anti-coordination-game]{anti-coordination game}, we introduce two types of \hyperref[def:game]{games}.

\begin{definition}[Attack-defense game]\label{def:attach-defense-game}
	An \emph{attack-defense game} can be simply described as follows. There are two \hyperref[def:player]{players}, one is attacker and another is defender.
	Attacker has two \hyperref[def:strategy]{actions}, they are attack with \hyperref[def:strategy]{strategy} \(A\) or \(B\), respectively.
	Correspondingly, defender also has two \hyperref[def:strategy]{actions} to choose from, namely defend against \(A\) or defend against \(B\). Clearly,
	if defender correctly defend attacker's attack in terms of \hyperref[def:strategy]{strategy} attack chooses, then defender gets a higher \hyperref[def:reward]{payoff},
	otherwise the attacker gets a higher \hyperref[def:reward]{payoff}.
\end{definition}

\begin{definition}[Zero-sum game]\label{def:zero-sum-game}
	A \emph{zero-sum game} is a \hyperref[def:game]{game} such that the sum of \hyperref[def:reward]{rewards} in each outcome is always zero.
\end{definition}

\begin{eg}[Rock, paper, scissors]\label{eg:rock-paper-scissors}
	This is an example that a \hyperref[def:game]{game} without \hyperref[def:pure-strategy]{pure} \hyperref[def:Nash-equilibrium]{Nash equilibrium}.
	The \hyperref[def:payoff-matrix]{payoff matrix} is
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|c|}
			                        & \multicolumn{1}{c}{} & \multicolumn{3}{c}{Player 2}                                                                                   \\
			                        & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$\text{Rock}$} & \multicolumn{1}{c}{$\text{Paper}$} & \multicolumn{1}{c}{$\text{Scissors}$} \\\cline{3-5}
			\multirow{3}*{Player 1} & $\text{Rock}$        & $(0, 0)$                          & $(-1, \underline{+1})$             & $(\underline{+1}, -1)$                \\\cline{3-5}
			                        & $\text{Paper}$       & $(\underline{+1}, -1)$            & $(0, 0)$                           & $(-1, \underline{+1})$                \\\cline{3-5}
			                        & $\text{Scissors}$    & $(-1, \underline{+1})$            & $(\underline{+1}, -1)$             & $(0, 0)$                              \\\cline{3-5}
		\end{tabular}
	\end{table}
	And it's clear that no matter what we fixed a \hyperref[def:strategy]{strategy} for one \hyperref[def:player]{player} first, another \hyperref[def:player]{player}
	will have incentive to deviate right after another \hyperref[def:player]{player} changes his \hyperref[def:strategy]{strategy}.
\end{eg}

\begin{eg}[Hawk-Dove]\label{def:Hawk-Dove}
	Two neighboring countries are likely going to fight. If one choose Hawk (\(H\)), then that country will fight; otherwise if choosing Dove (\(D\)), then that country will not
	going to fight and will not going to fight back either.
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|}
			                         & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Country 2}                           \\
			                         & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$H$}       & \multicolumn{1}{c}{$D$} \\\cline{3-4}
			\multirow{2}*{Country 1} & $H$                  & $(0, 0)$                      & $(5, 1)$                \\\cline{3-4}
			                         & $D$                  & $(1, 5)$                      & $(3, 3)$                \\\cline{3-4}
		\end{tabular}
	\end{table}
	\begin{problem}
	Is \((D, D)\) a \hyperref[def:Nash-equilibrium]{Nash Equilibrium}?
	\end{problem}
	\begin{answer}
		No. Since if \hyperref[def:player]{player} 2 is playing \(D\), we see that \(u_1(H, D) = 5\), \(u_1(D, D) = 3\). Then \hyperref[def:player]{player} \(1\) has a
		deviation to get a higher \hyperref[def:reward]{payoff}.
	\end{answer}
	\begin{problem}
	Is \((H, D)\) a \hyperref[def:Nash-equilibrium]{Nash Equilibrium}?
	\end{problem}
	\begin{answer}
		True. Since
		\begin{itemize}
			\item \hyperref[def:player]{Player} 1: \(H\) is the \hyperref[def:best-response]{best response}.
			\item \hyperref[def:player]{Player} 2: \hyperref[def:player]{Player} 1 is \(H\). Then \(u_2(H, H) = 0\), and \(u_2(H, D) = 1\).
		\end{itemize}
		We see that neither \hyperref[def:player]{player} 1 nor \hyperref[def:player]{player} 2 has an incentive to deviate. So \((D, H)\) (also \((H, D)\)) is a
		\hyperref[def:Nash-equilibrium]{Nash Equilibrium}.
	\end{answer}
\end{eg}

\begin{eg}[Matching Pennies]\label{eg:matching-pennies}
	This is a simple example for \hyperref[def:attach-defense-game]{attack-defense game} and also a \hyperref[def:zero-sum-game]{zero-sum game}. Suppose there are
	two people each hold a penny and simultaneously choose  whether to show heads (\(H\)) or tails (\(T\)) on their penny. \hyperref[def:player]{Player} 1 loses
	his penny to \hyperref[def:player]{player} 2 if they match; \hyperref[def:player]{player} 1 wins \hyperref[def:player]{player} 2's penny if they don't match.
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|}
			                        & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Player 2}                           \\
			                        & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$H$}      & \multicolumn{1}{c}{$T$} \\\cline{3-4}
			\multirow{2}*{Player 1} & $H$                  & $(-1, 1)$                    & $(1, -1)$               \\\cline{3-4}
			                        & $T$                  & $(1, -1)$                    & $(-1, 1)$               \\\cline{3-4}
		\end{tabular}
	\end{table}
	We see that there are no \hyperref[def:pure-strategy]{pure strategy} \hyperref[def:Nash-equilibrium]{Nash equilibrium}. So we have no idea what
	\hyperref[def:strategy]{strategy} each \hyperref[def:player]{player} will play.
	\begin{remark}
		This is also a \hyperref[def:zero-sum-game]{zero-sum game}.
	\end{remark}
\end{eg}

\begin{remark}
	From \hyperref[eg:matching-pennies]{matching pennies games} example, we see that there are no \hyperref[def:pure-strategy]{pure strategy}
	\hyperref[def:Nash-equilibrium]{Nash Equilibrium}. This is due to
	the fact that all \hyperref[def:player]{player} will play their \hyperref[def:strategy]{strategy} in a deterministic way, if the \hyperref[def:best-response]{best response}
	exists. And if not, then we have no idea what they will play in this case, which is hard to analyze.
	Now if we allow some randomness, then by \href{https://en.wikipedia.org/wiki/John_Forbes_Nash_Jr.}{John Nash}, he proved that with such a setup, the
	\hyperref[def:Nash-equilibrium]{Nash Equilibrium} always exists (\autoref{thm:Nash-existence-theorem}).
\end{remark}

\subsection{Mixed Strategy Nash Equilibrium}
Follows the intuition of \hyperref[eg:matching-pennies]{matching pennies} game, we now go in-depth and analyze \hyperref[def:mixed-strategy]{mixed strategy}.
Essentially, we are just randomizing one's \hyperref[def:strategy]{strategies} such that the randomization is done \hyperref[def:independent]{independently}.
This will bring the \hyperref[def:mathematical-game]{game structure} with much more interesting properties.

\begin{prev}
	The reason why we call it \hyperref[def:mixed-strategy]{mixed strategies} is initially, we only have distinct \hyperref[def:strategy]{strategy} to choose from.
	But now, with probability, we have some kind of \textbf{mixed} \hyperref[def:strategy]{strategy} between choices. In \hyperref[eg:matching-pennies]{matching pennies game}
	example, we have \hyperref[def:strategy]{strategies} which mixes between \(H\) and \(T\).
\end{prev}

\begin{eg}
	For \hyperref[def:player]{player} \(i\), denote \(\overrightarrow{P}_{i}\) as a probability distribution on \(\mathcal{S}_i\) (PMF) such that
	\[
		\left\{s_{i1}, s_{i2}, s_{i3}, s_{i4}\right\} = \mathcal{S}_i,
	\]
	and \(S_i = 4\). More specifically, \(\overrightarrow{P}_i\) looks like
	\[
		(P_{i1}, P_{i2}, P_{i3}, P_{i4})
	\]
	with all entries non-negative and their sum is \(1\). There are lots of examples for such \(P_{i}\). For example,
	\begin{itemize}
		\item Uniformly \hyperref[def:mixed-strategy]{mixed}: \((\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4})\).
		\item \hyperref[def:pure-strategy]{Pure strategy}: \((1, 0, 0, 0)\).
	\end{itemize}

	We can then consider so-called \hyperref[def:probability-simplex]{probability simplex}, defined as follows.

	\begin{definition}[Probability simplex]\label{def:probability-simplex}
		The \emph{probability simplex}, denotes as 	\(\Delta(\mathcal{S}_i )\), is equal to
		\[
			\left\{\left(P_{i1}, P_{i2}, \ldots , P_{i S_{i}}\right)\colon P_{ij}\geq 0\land \sum\limits_{j=1}^{S_i} P_{ij} = 1\quad \forall j = 1, \ldots , S_{i}\right\}.
		\]
		\begin{figure}[H]
			\centering
			\incfig{probability-simplex}
			\label{fig:probability-simplex}
		\end{figure}
	\end{definition}
	\begin{intuition}
		This is just a set of probability distributions over \(\mathcal{S}_i\), but with a good geometry representation.
	\end{intuition}
\end{eg}

\hyperref[def:player]{Players}' \hyperref[def:strategy]{actions} are enhanced to picking probability distribution (adding uncertainty to your choice).

\begin{problem}
How to evaluate \hyperref[def:reward]{payoff}?
\end{problem}
\begin{answer}
	Let
	\[
		u_{i}(\overrightarrow{P}_i, \overrightarrow{P}_{-i})
		\expectation{\overrightarrow{P}_{1}\times \overrightarrow{P}_2 \times \ldots \times \overrightarrow{P}_I}{u_{i}\left(\overline{S}_1, \overline{S}_2, \ldots , \overline{S}_I\right)},
	\]
	such that \(\overline{S}_i\) is a \hyperref[def:random-variable]{random variable} that takes values in \(\mathcal{S}_i\) with
	distribution \(\overrightarrow{P}_i\), and they are \hyperref[def:independent]{mutually independent}. This is further equal to
	\[
		\sum\limits_{j_1\in \mathcal{S}_1 }\sum\limits_{j_2\in \mathcal{S}_2}\ldots \sum\limits_{j_I\in \mathcal{S}_I }P_{1j_1}\times P_{2j_2}\times \ldots \times P_{Ij_I}\times u_{i}\left(\overline{S}_{1j_1}, \overline{S}_{2j_2}, \ldots , \overline{S}_{Ij_I}\right).
	\]
\end{answer}

\begin{eg}[Matching Pennies revisit]
	Recall the \hyperref[eg:matching-pennies]{matching pennies game}.
	\begin{table}[H]
		\centering
		\setlength{\extrarowheight}{2pt}
		\begin{tabular}{cc|c|c|}
			                                               & \multicolumn{1}{c}{} & \multicolumn{2}{c}{\hyperref[def:player]{Player} 2}                           \\
			                                               & \multicolumn{1}{c}{} & \multicolumn{1}{c}{$H$}                             & \multicolumn{1}{c}{$T$} \\\cline{3-4}
			\multirow{2}*{\hyperref[def:player]{Player} 1} & $H$                  & $(1, -1)$                                           & $(-1, 1)$               \\\cline{3-4}
			                                               & $T$                  & $(-1, 1)$                                           & $(1, -1)$               \\\cline{3-4}
		\end{tabular}
	\end{table}
	with \(P_1 = (\frac{3}{4}, \frac{1}{4})\) and \(P_2 = (\frac{1}{3}, \frac{2}{3})\). Then
	\[
		\begin{split}
			u_1(P_1, P_2) &= \frac{3}{4}\times \frac{1}{3} u_1(H, H) + \frac{3}{4}\times \frac{2}{3}u_1(H, T) + \frac{1}{3}\times \frac{1}{3}u_1(T, H)+\frac{1}{4}\times \frac{2}{3}u_1(T, T)\\
			&= \frac{3}{12} - \frac{6}{12} - \frac{1}{12} + \frac{2}{12}\\
			&= -\frac{1}{6}.
		\end{split}
	\]
	Also, \(u_2(P_1, P_2)\) is \(\frac{1}{6}\) from the \hyperref[def:zero-sum-game]{zero-sum setting}.
\end{eg}

We can now combine the probability set up with the definition of the \hyperref[def:best-response]{best response}, which gives us
\[
	\mathrm{BR}_{i}(\overrightarrow{P}_{-i}) = \left\{\overrightarrow{P}_i\in \Delta(\mathcal{S}_i)\colon u_{i}(\overrightarrow{P}_i, \overrightarrow{P}_{-i}) = \max_{\hat{P}_i\in \Delta(\mathcal{S}_i)}u_{i}(\hat{P}_i, \overrightarrow{P}_{-i})\right\}.
\]
Also, for Nash Equilibrium, let \(P^{\ast}\) be the same probability distribution for all \(i\in \mathcal{I} \), then
\[
	P^{\ast}_i\in\mathrm{BR}_i(P^{\ast}_{-i})\iff \underset{P_{i}\in \Delta(\mathcal{S}_i)}{\forall }\ u_{i}(P^{\ast}_i, P^{\ast}_{-i})\geq u_{i}(P_{i}, P^{\ast}_{-i}).
\]

\begin{theorem}[Nash's existence theorem]\label{thm:Nash-existence-theorem}
	Every finite \hyperref[def:mathematical-game]{game} (finite number of \hyperref[def:strategy]{actions}) has at least one \hyperref[def:mixed-strategy]{mixed strategy}
	\hyperref[def:Nash-equilibrium]{Nash Equilibrium}.
\end{theorem}
\begin{proof}
	A nice short proof can be found \href{https://www.cs.ubc.ca/~kevinlb/papers/TR-2007-25.pdf}{here}. It's essentially proved by
	\href{https://en.wikipedia.org/wiki/Brouwer_fixed-point_theorem}{Brouwer fixed-point theorem}.
\end{proof}