\lecture{13}{20 Oct. 12:30}{Stochastic Process and Markov Chain}
Let's first see some examples of \hyperref[def:i.i.d.]{i.i.d.} \hyperref[def:stochastic-process]{stochastic process}, where \(i\) is the time index unspecified.
\begin{eg}[Biased coin toss]
	For all \(i\), let \(X_{i}\in\{0, 1\}\) and
	\[
		\probability{}{X_{i} = 1} = p.
	\]
	We denote this as \(X_i\sim \mathrm{Beronulli}(p)\).
\end{eg}

\begin{eg}[Dice toss]
	For all \(i\), let \(X_{i}\in\{1, 2, 3, 4, 5, 6\}\) and
	\[
		\probability{}{X_{i} = j} = \frac{1}{6}.
	\]
	We denote this as \(X_i\sim \mathrm{Uniform}(\{1, 2, 3, 4, 5, 6\})\).
\end{eg}

\begin{eg}[Uniform distribution]
	For all \(i\), let \(X_i\sim \mathrm{Uniform}(\left[ 0, 1 \right] )\), where \(X_{i}\in\left[ 0, 1 \right] \).
\end{eg}

\begin{eg}[Exponential distirbution]
	For all \(i\), let \(X_i\sim \mathrm{exp}(\lambda)\), then
	\[
		\probability{}{X_{i}>u} = e^{-\lambda u}\qquad \forall u\geq 0.
	\]
\end{eg}

\begin{eg}[Normal distribution]
	For all \(i\), let \(X_i\sim \mathcal{N}(\mu, \sigma^2)\).
\end{eg}

\begin{remark}
	Notice that our discussion will focus on \(\expectation{}{X_i} < +\infty \).
\end{remark}

The main reason we study \hyperref[def:i.i.d.]{i.i.d.} \hyperref[def:stochastic-process]{stochastic process} is because of the following theorem.
\begin{theorem}[Strong law of large numbers]\label{thm:SLLN}
	For an \hyperref[def:i.i.d.]{i.i.d.} sequence \(\{X_i\}\), it follows that
	\[
		\lim_{n \to \infty} \frac{1}{n}\sum\limits_{i=1}^{n} X_{i} = \expectation{}{X}.
	\]
	This is so-called the \emph{strong law of large numbers} (SLLN).
\end{theorem}

\begin{eg}[Coin toss]
	If an unfair coin will face up with probability \(p\), then \hyperref[thm:SLLN]{strong law of large numbers} tells us that
	\[
		\frac{\# \mathrm{H} }{\# \mathrm{H} + \# \mathrm{T} } \cong p
	\]
	with many enough tosses. Mathematically, we let \(Y_{i} = \mathbbm{1}_{\{X_{i}\in B\}}\), where
	\[
		\mathbbm{1}_{\{X_{i}\in B\}}(\omega) = \begin{dcases}
			0, & \text{ if }X_{i}(\omega)\in B; \\
			1, & \text{ otherwise},
		\end{dcases}
	\]
	hence \(y_{i}\sim \mathrm{Bernoulli}(p = \probability{}{X_{i}\in B})\). Then we can use the \hyperref[def:i.i.d.]{i.i.d.} sequence \(Y_i\) to
	describe a series of coin tosses.
\end{eg}

\subsection{Conditional Property}
\begin{prev}
	We first note that if \(\probability{}{A\cap B} = \probability{}{A} \probability{}{B} \), then \(A\) and \(B\) are \hyperref[def:independent]{independent}.
\end{prev}

Now, if \(A\) and \(B\) are dependent, then given \(B\) is happened, we should be able to say something about the probability under the current condition.
This suggests the following definition.

\begin{definition}[Conditional probability]\label{def:conditional-probability}
	Let \(\probability{}{B}>0\), then the \emph{conditional probability} of event \(A\) under condition \(B\) is defined as
	\[
		\probability{}{A \mid B}\coloneqq \frac{\probability{}{A\cap B}}{\probability{}{B}}.
	\]
\end{definition}

Similarly, we have the \hyperref[def:conditional-expectation]{conditional expectation}
\begin{definition}[Conditional expectation]\label{def:conditional-expectation}
	The \emph{conditional expectation} is defined as
	\[
		\expectation{}{X \mid X\in A} = \expectation{}{X \mid A}.
	\]
\end{definition}

\begin{eg}
	We look at an example for discrete \hyperref[def:random-variable]{random variable} \(X\). Let \(X\in \Omega\), then
	\[
		\begin{split}
			\expectation{}{X \mid X\in A} &= \sum\limits_{\omega\in \Omega} \omega\probability{}{X = \omega \mid X\in A}\\
			&=\sum\limits_{\omega\in \Omega} \omega \frac{\probability{}{\left\{X = \omega\right\}\cap \left\{x\in A\right\}}}{\probability{}{X\in A} }\\
			&=\sum\limits_{\omega\in \Omega} \omega \frac{\probability{}{X = \omega}}{\probability{}{X\in A} }\\
			&=\frac{\sum\limits_{\omega\in \Omega} \omega \probability{}{X = \omega}}{\probability{}{X\in A}}
		\end{split}
	\]
\end{eg}

Now, consider three events \(A, B, C\), then we have
\[
	\probability{}{X\in A, X\in B, X\in C} \coloneqq \probability{}{X\in A\cap B\cap C}.
\]
Then if \(A, B,  C\) are \hyperref[def:independent]{independent}, then we further have
\[
	\probability{}{X\in A\cap B\cap C}= \probability{}{X\in A}\probability{}{X\in B}\probability{}{X\in C}
\]
as we have seen. If they are dependent, given \(\probability{}{X\in C}>0\), then
\[
	\probability{}{X\in A, X\in B, X\in C} = \probability{}{X\in A, X\in B \mid X\in C}\probability{}{X\in C}.
\]
This suggests the notion of \hyperref[def:conditionally-independent]{conditional independence} as follows to help us further factorize the above equation.
\begin{definition}[Conditionally independent]\label{def:conditionally-independent}
	For events \(A, B, C\), if
	\[
		\probability{}{X\in A, X\in B \mid X\in C} = \probability{}{X\in A \mid X\in C}\probability{}{X\in B \mid X\in C},
	\]
	then \(A\) and \(B\) are \emph{conditionally independent} given \(C\).
\end{definition}

\section{Markov Chain}
We now formally define \hyperref[def:Markov-chain]{Markov chain}.
\begin{definition}[Markov chain]\label{def:Markov-chain}
	A \emph{Markov chain} is a dependent \hyperref[def:stochastic-process]{stochastic process} with a specific form of \hyperref[def:conditionally-independent]{conditional independence}
	defined as follows. Consider a \hyperref[def:random-variable]{random variables} sequence
	\[
		X_1, X_2, X_3, \ldots
	\]
	with discrete time index \(1, 2, 3, \ldots \).\footnote{Noting that they lie on a \underline{discrete space}, like real number, labels, etc.}
	At time \(n>1\), we have \(X_n = a_n\) where \(a_n\) takes values in a discrete set. Then
	\[
		\begin{split}
			&\probability{}{X_1 = a_1, X_2 = a_2,  \ldots X_{n-1} = a_{n-1}, X_{n+1} = a_{n+1},\ldots , X_{n+m} = a_{n+m}  \mid X_n = a_n }\\
			=&\probability{}{X_1 = a_1, X_2 = a_2,  \ldots X_{n-1} = a_{n-1}\mid X_n = a_n }\\
			&\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\cdot \probability{}{X_{n+1} = a_{n+1},\ldots , X_{n+m} = a_{n+m}  \mid X_n = a_n}.
		\end{split}
	\]
\end{definition}

\begin{intuition}[Conditional independence property of Markov chains]
	Knowing where we are, the progress in the future is \hyperref[def:conditionally-independent]{conditionally independent} of how we got to the present, i.e.,
	knowing the present, the past and the future are \hyperref[def:conditionally-independent]{conditionally independent}.
\end{intuition}

\begin{note}
	\hyperref[def:Markov-chain]{Markov chains} will not be \hyperref[def:i.i.d.]{i.i.d.} sequences of \hyperref[def:random-variable]{random variables}
	since there will be dependence. But still, the \hyperref[thm:SLLN]{strong law of large number} will hold under conditions.
\end{note}

\subsection{Time Homogeneous Markov Chain}
We need to know two things:
\begin{itemize}
	\item initial distribution \(X_0 \sim \mu\) (or \(X_1\sim \mu\))
	\item one-step transition matrix
	      \[
		      P_{ij} \coloneqq \probability{}{X_{n+1} = j \mid X_n = i}\qquad \forall i, j.
	      \]


	      \begin{note}
		      Note that
		      \begin{enumerate}
			      \item Notice that this is not a function of \(n\).
			      \item The condition here is just for an example from the lecture.\todo{Review the example in lecture}
		      \end{enumerate}
	      \end{note}

	      Then we see that the total probability of \(\probability{}{X_n = i}\) is
	      \[
		      \begin{split}
			      \probability{}{X_n = i} &= \sum_j \probability{}{X_n = i, X_0 = j}\\
			      &= \sum_j \probability{}{X_0 = j}\probability{}{X_n = i  \mid  X_0 = j}\\
			      &=\sum_j \mu_{j} (P^n)_{ji}\\
			      &= (\mu^{\top} P^n)_i
		      \end{split}
	      \]
	      where the first step is from the definition of \emph{total probability}. Recall that
	      \[
		      P_{ij} = \probability{}{X_{n+1} = j \mid X_n = i},
	      \]
	      hence
	      \[
		      \begin{split}
			      \sum_{j\in \Omega} P_{ij} &= \sum_{j\in \Omega} \probability{}{X_{n+1} = j \mid X_n = i}\\
			      &= \probability{}{X_{n+1}\in \Omega \mid X_n = i}\\
			      &= \frac{\probability{}{X_{n+1}\in \Omega, X_n = i} }{\probability{}{X_n = i} }
			      = \frac{\probability{}{X_n = i} }{\probability{}{X_n = i} }
			      = 1.
		      \end{split}
	      \]
	      We see that \(P\) is row stochastic and \(P_{i\cdot}\) is a probability distribution on \(\Omega\). So \(\rho(P) = 1\implies \exists  \pi\)(distribution) such that
	      \[
		      \pi^{\top} P = \pi^{\top}.
	      \]
	      We see that the left \hyperref[def:eigenvector]{eigenvector} is a distribution. We have \(P\) is irreducible and \(\Omega\) is finite, then
	      \[
		      \lim_{n\to \infty } \frac{1}{n}\underline{\sum\limits_{m=1}^{n}  \mathbbm{1}_{\left\{ X_m = i \right\} }} = \pi_i
	      \]
	      almost surely.
\end{itemize}

Refinement with aperiodic \(P\),
\[
	\lim_{n\to \infty }\probability{}{X_n = i} = \pi_i
\]